{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Test CLD\n",
    "Notebook for various tests of the pycld2 library"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from pycld2 import detect\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data from Wiki files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = {}\n",
    "for n in [\"train\", \"val\", \"test\"]:\n",
    "    nowiki = [(line.strip(), \"no\") for line in open(f\"../res/wiki/nowiki-{n}.txt\")]\n",
    "    nnwiki = [(line.strip(), \"nn\") for line in open(f\"../res/wiki/nnwiki-{n}.txt\")]\n",
    "\n",
    "    m = min(len(nowiki), len(nnwiki))\n",
    "    sample = random.sample(nowiki, m)\n",
    "    sample += random.sample(nnwiki, m)\n",
    "\n",
    "    random.shuffle(sample)\n",
    "\n",
    "    data[n] = sample\n",
    "\n",
    "x_train, y_train = zip(*data[\"train\"])\n",
    "x_val, y_val = zip(*data[\"val\"])\n",
    "x_test, y_test = zip(*data[\"test\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data from WiLI dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wili_train = open(\"../res/wili-2018/x_train.txt\"), open(\"../res/wili-2018/y_train.txt\")\n",
    "wili_test = open(\"../res/wili-2018/x_test.txt\"), open(\"../res/wili-2018/y_test.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../res/wili-2018/labels.csv\", delimiter=\";\")\n",
    "\n",
    "pred = []\n",
    "true = []\n",
    "for text, label in wili_test:\n",
    "    text = re.sub(\"[]\", \"\", text)  # Clean text so cld doesn't crash\n",
    "    text = text.strip()\n",
    "    label = label.strip()\n",
    "    try:\n",
    "        d = detect(text, returnVectors=True, bestEffort=True)\n",
    "        p = d[2][0][1]  # Number one prediction\n",
    "        \n",
    "        # Use wiki code from WiLI (matches better with cld codes)\n",
    "        t = df[df[\"Label\"] == label][\"Wiki Code\"].values\n",
    "        \n",
    "        # a few more adjustments for matching\n",
    "        t = t[0] if len(t) > 0 else \"other\"\n",
    "        conv = {\"arz\": \"ar\", \"tcy\": \"kn\", \"he\": \"iw\", \"xmf\": \"ka\", \"be-tarask\": \"be\", \"zh-classical\": \"zh\",\n",
    "                \"zh-yue\": \"zh\", \"jv\": \"jw\"}\n",
    "        t = conv[t] if t in conv else t\n",
    "        \n",
    "        if len(t) == 2 or t == p or t in {\"chr\", \"sco\", \"war\", \"zh-Hant\"}:\n",
    "            pred.append(p if p != \"zh-Hant\" else \"zh\")\n",
    "            true.append(t)\n",
    "    except Exception as e:\n",
    "        print(text)\n",
    "        print(e)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get stats."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counter = Counter((a, b) for a, b in zip(pred, true) if a != b and a != \"un\")\n",
    "\n",
    "print(counter.most_common())\n",
    "\n",
    "# for y in confusion_matrix(true, pred):\n",
    "#     print(list(y))\n",
    "print(confusion_matrix(true, pred))\n",
    "print(classification_report(true, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Additional speaker text identification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "files = os.listdir(\"../res/speaker_texts\")\n",
    "files = sorted(files, key=lambda x: x.lower().replace(\"æ\", \"{\").replace(\"ø\", \"|\").replace(\"å\", \"}\"))\n",
    "\n",
    "d = open(\"speakers.csv\", \"w\")\n",
    "d.write(\"file,is_reliable,bytes,\" + \",\".join(\n",
    "    (f\"lang{i}_name,lang{i}_code,lang{i}_percentage,lang{i}_score\" for i in range(3))) + \"\\n\")\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    txt = open(f\"../res/speaker_texts/{f}\").read()\n",
    "    txt = re.sub(r\"(^|\\n)[^\\t]+\\t\", \"\", txt)\n",
    "    det = detect(txt, hintTopLevelDomain=\"no\")\n",
    "\n",
    "    d.write(f + \",\" + re.sub(r\"[\\s()]+\", \"\", str(det)) + \"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}